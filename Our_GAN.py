# -*- coding: utf-8 -*-
"""Generative_Adversarial_Networks_PyTorch.ipynb

Automatically generated by Colaboratory.

## Setup Example
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/My Drive/Colab Notebooks')

!pip3 install tensorboardX

import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd.variable import Variable
from torchvision import transforms
from torchvision.datasets import MNIST
from torchvision.utils import make_grid
from torch.utils.data import DataLoader
import imageio
from utils import Logger
...

"""## Dataset
To simplify, the PyTorch MNIST wrapper, which downloads and loads the MNIST dataset. See the [documentation](https://github.com/pytorch/vision/blob/master/torchvision/datasets/mnist.py) for more information about the interface. The default parameters will take 5,000 of the training examples and place them into a validation dataset. The data will be saved into a folder called `MNIST_data`.
"""

batch_size = 5000 #number of training examples
transform_data = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.5], std=[0.5])])

mnist_data = MNIST(root='./MNIST_data', train=True, download=True, transform=transform_data) 
data_loader = DataLoader(dataset=mnist_data, shuffle=True, batch_size=batch_size)

"""## Random Noise
Generate uniform noise from -1 to 1 with shape `[batch_size, dim]`. Implement `sample_noise` Hint: use `torch.rand`. Make sure noise is the correct shape and type:
"""

def sample_noise(batch_size, dim): 
    return torch.rand(batch_size, dim) + torch.rand(batch_size, dim)*(-1)

"""# Discriminator
Our first step is to build a discriminator. Fill in the architecture: A three hidden-layer discriminative neural network.
 * Fully connected layer 
 * LeakyReLU 
 * Fully connected layer 
 * LeakyReLU 
 * Fully connected layer
"""

class DiscriminatorNet(torch.nn.Module):

    def __init__(self):
        super(DiscriminatorNet, self).__init__()
        input_size = 784 #28x28
        DfirstHiddenLayer_size = 1024
        DsecondHiddenLayer_size = 512
        DthirdHiddenLayer_size = 256
        
        output_size = 1
        
        self.firstHiddenLayer = nn.Sequential( 
            nn.Linear(input_size, firstHiddenLayer_size),
            nn.LeakyReLU(0.2)
        )
        self.secondHiddenLayer = nn.Sequential(
            nn.Linear(firstHiddenLayer_size, secondHiddenLayer_size),
            nn.LeakyReLU(0.2)
        )
        self.thirdHiddenLayer = nn.Sequential(
            nn.Linear(secondHiddenLayer_size, thirdHiddenLayer_size),
            nn.LeakyReLU(0.2)
        )
        self.outputLayer = nn.Sequential(
            torch.nn.Linear(thirdHiddenLayer_size, output_size),
            torch.nn.Sigmoid()
        )

    def forward(self, x):
        x = self.firstHiddenLayer(x)
        x = self.secondHiddenLayer(x)
        x = self.thirdHiddenLayer(x)
        x = self.outputLayer(x)
        return x
    
discriminator = DiscriminatorNet()

"""# Generator
Similar like above:
 * Fully connected layer
 * `ReLU`
 * Fully connected layer
 * `ReLU`
 * Fully connected layer
 * `TanH` (to clip the image to be in the range of [-1,1])
"""

class GeneratorNet(torch.nn.Module):

    def __init__(self):
        super(GeneratorNet, self).__init__()
        n_features = 6000 #not sure what the correct value would be, is this how many photos we have?
        n_out = 784 #I think 784 is correct? 
        GfirstHiddenLayer_size = 256
        GsecondHiddenLayer_size = 512
        GthirdHiddenLayer_size = 1024
        
        #all values will probably have to be replaced, I don't know how to check if this is correct
        self.hidden0 = nn.Sequential(
            nn.Linear(n_features, GfirstHiddenLayer_size), 
            nn.LeakyReLU(0.2)
        )
        self.hidden1 = nn.Sequential(            
            nn.Linear(GfirstHiddenLayer_size, GsecondHiddenLayer_size), 
            nn.LeakyReLU(0.2)
        )
        self.hidden2 = nn.Sequential(
            nn.Linear(GsecondHiddenLayer_size, GfirstHiddenLayer_size), 
            nn.LeakyReLU(0.2)
        )
        
        self.out = nn.Sequential(
            nn.Linear(GfirstHiddenLayer_size, n_out),
            nn.Tanh()
        )

    def forward(self, x):
        x = self.hidden0(x)
        x = self.hidden1(x)
        x = self.hidden2(x)
        x = self.out(x)
        return x
    
generator = GeneratorNet()

"""# Optimization
Make a function that returns an `optim.Adam` optimizer
"""

# Optimizers
Discriminator_lr = 0.0007
Generator_lr = 0.0006
Discriminator_Optimizer = optim.Adam(discriminator.parameters(), lr=Discriminator_lr) #lr = learning rate
Generator_Optimizer = optim.Adam(generator.parameters(), lr=Generator_lr)

# Loss function
loss = nn.BCELoss()

"""# Training a GAN!"""

def real_data_target(size):
    data = Variable(torch.ones(size, 1))
    return data

def fake_data_target(size):
    data = Variable(torch.zeros(size, 1))
    return data

def train_discriminator(optimizer, real_data, fake_data):
    N = real_data.size(0)
    # Reset gradients
    optimizer.zero_grad()
   
    #Training on real data
    prediction_real = discriminator(real_data)
    #Calculate error and backpropagation
    error_real = loss(prediction_real,real_data_target(N))
    error_real.backward()
    
    #Training on fake data
    prediction_fake = discriminator(fake_data)
    #Calculate error and backpropagation
    error_fake = loss(prediction_fake,fake_data_target(N))
    error_fake.backward()
    
    #Update weights with gradients
    optimizer.step()
    
    # Return error and prediction for real and fake inputs
    return error_real + error_fake,prediction_real,prediction_fake

def train_generator(optimizer, fake_data):
    N = fake_data.size(0)
    #Reset gradients
    optimizer.zero_grad()
    # Generate fake data
    prediction = discriminator(fake_data)
    # Calculate error and backpropagate
    error = loss(prediction,real_data_target(N))
    error.backward()
    # Update weights with gradients
    optimizer.step()
    
    return error

logger = Logger(model_name='VGAN', data_name='MNIST')

for epoch in range(num_epochs):
    for n_batch, (real_batch,_) in enumerate(data_loader):

        # Train Discriminator
        # Generate fake data
        # Train D


        # Train Generator
        # Generate fake data
        # Train G
        # Log error

        # Model Checkpoints
        logger.save_models(generator, discriminator, epoch)
